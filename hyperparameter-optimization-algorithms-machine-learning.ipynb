{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04db116c",
   "metadata": {},
   "source": [
    "# Tables of Content:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff200f",
   "metadata": {},
   "source": [
    "Hyperparameter Optimization: A Short Review of Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8041715",
   "metadata": {},
   "source": [
    "# INTRODUCTION \n",
    "Optimization is one of the core components of machine learning. The essence of most machine learning algorithms is to build an optimization model and learn the parameters in the objective function from the given data.\n",
    "\n",
    "Almost all machine learning algorithms can be formulated as an optimization problem to find the extremum of an objective function. Building models and constructing reasonable objective functions are the first step in machine learning methods. With the determined objective function, appropriate numerical or analytical optimization methods are usually used to solve the optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a75c7f",
   "metadata": {},
   "source": [
    "For supervised learning, the goal is to find an optimal\n",
    "mapping function $f(x)$ to minimize the loss function of the\n",
    "training samples,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae767f0",
   "metadata": {},
   "source": [
    "$$\n",
    "min_{\\theta}\\frac{1}{N}\\sum_{i=1}^{N}L(y^{i},f(x^{i},\\theta))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99267eef",
   "metadata": {},
   "source": [
    "where N is the number of training samples, Î¸ is the parameter\n",
    "of the mapping function, xi is the feature vector of the ith\n",
    "samples, y i is the corresponding label, and L is the loss\n",
    "function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadde26e",
   "metadata": {},
   "source": [
    "Many of these parameters affect the model's quality in predicting results from new input data. These parameters are intrinsic to the models as they represent their characteristics, such as the number of interactions, regularization term, tree size, etc. These parameters need to be chosen before the algorithm's learning process begins and are referred to as hyperparameters. The process of selecting these parameters to improve the model's performance is known as hyperparameter tuning (GOODFELLOW; BENGIO; COURVILLE, 2016; KUHN; JOHNSON, 2013; PROBST; BISCHL; BOULESTEIX, 2018; YANG; SHAMI, 2020; HUTTER; KOTTHOFF; VANSCHOREN, 2019; ZHENG, 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281e822",
   "metadata": {},
   "source": [
    "The process of tuning hyperparameters is generally treated as a black-box optimization problem. Formally, we can define the problem as follows: Let:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af2abf",
   "metadata": {},
   "source": [
    "\n",
    "The process of tuning hyperparameters is generally treated as a black-box optimization problem. Formally, we can define the problem as follows:\n",
    "$$\n",
    "    \\mathcal{H} = \\mathcal{H}_1 \\times \\mathcal{H}_2 \\times ... \\times \\mathcal{H}_k\n",
    "$$\n",
    "where $\\mathcal{H}$ is the hyperparameter space and an algorithm $a \\in \\mathcal{A}$, where $\\mathcal{A}$ is a set of machine learning algorithms. Each $\\mathcal{H}_i$ represents a set of possible values for the $i^{th}$ hyperparameter of $a$ ($i \\in {1, ..., k}$). Let $\\mathcal{D}$ be a dataset, where $D \\in \\mathcal{D}$ is a specific dataset in $\\mathcal{D}$. The function $f: \\mathcal{A} \\times \\mathcal{D} \\times \\mathcal{H} \\rightarrow \\mathbb{R}$ calculates the model's performance using algorithm $a \\in \\mathcal{A}$ on dataset $D \\in \\mathcal{D}$ given the hyperparameter configuration $h = (h_1, h_2, ..., h_k) \\in \\mathcal{H}$.\n",
    "\n",
    "Given $a \\in \\mathcal{A}$, $\\mathcal{H}$, and $D \\in \\mathcal{D}$, the objective of tuning the hyperparameters is to find $h^* = (h_1^, h_2^, ..., h_k^*)$ such that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2760594",
   "metadata": {},
   "source": [
    "It often requires deep knowledge of machine learning algorithms and appro-\n",
    "priate hyper-parameter optimization techniques. Although several automatic\n",
    "optimization techniques exist, they have different strengths and drawbacks\n",
    "when applied to different types of problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210d210",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff516e3",
   "metadata": {},
   "source": [
    "![](img/grid_layout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed6ec0",
   "metadata": {},
   "source": [
    "# Random search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858430b7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8131c2d3",
   "metadata": {},
   "source": [
    "![](img/random_layout.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
